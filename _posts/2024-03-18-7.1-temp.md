### Presentation Summary

---
- black box attack summary
- Pitfalls of previous adversarial models
  - high number of required queries
  - complexity of optimization process
- overcoming drawbacks
  - query-efficient

### Discussion Summary

---
- This paper gives more formal treatment about adversarial attack resources and functionalities (provides more guarantees)
  - Simplest level is attackers want to minimize the detection rate
  - To do that attackers need to modify the sample (easiest way is to add more content to the file)
  - Multi-objective function: minimize detection rate while also minimizing the size of the payload. Would also want to minimize the number of queries as excessive amounts of queries can raise a flag.
  - Minimizing one component raises the other(s), so there needs to be a balance
  - These problems can grow exponentially and be hard to minimize. Thus, options to find the optimal solution is to find heuristics, approximate, etc.
  - The key message of the paper is that malware detection can be seen as a multi-objective function
- Need to preserve the functionality when minimizing the objective (for instance removing the encryption of ransomware defeats the purpose of it being malicious)
- Whatever modification you do in the feature space (mathematically) always maps to the problem space in the real world
  - Modify regions of the binary that do not affect the binary's functionality (eg. adding something to the end of the binary)
  - Removal from binaries is a problem, you end up breaking the binary
  - Adding is more suggested
  - The paper suggests lex space?: which are spaces of the binary that are safe to modify, so add bytes to those spaces
  - If there are no lex spaces, create those spaces
  - Can open up space in the middle of a binary as the main entry point of binary execution, let's say C language for example, is the C library directory
- Soft labels vs hard labels attacks
  - Two other defined attacks, aside from white box and black box
  - Information pulled from the classifiers
  - Hard labels tell you if a sample is malicious or not
  - Soft labels tell you whether a sample is malicious or not, and a confidence level
  - As a defense, to defend against these attacks you can:
    - Provide less information publicly (less information about the models, using hard labels, etc.)
    - Soft labels provide attackers with feedback information, so use a pool of models. One model provides a correct confidence label, then another provides another confidence label during the next query
    - This information is still useful for users, but cannot help the attacker infer anything
    - This defense is called Moving Target
