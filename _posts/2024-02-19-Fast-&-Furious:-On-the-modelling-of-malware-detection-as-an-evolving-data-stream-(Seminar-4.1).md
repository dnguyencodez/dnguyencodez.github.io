## Fast & Furious: On the modelling of malware detection as an evolving data stream (Seminar 4.1)

**NOTE**: This blog is for a special topics course at Texas A&M (ML for Cyber Defenses). During each lecture a student presents information from the assigned paper. This blog summarizes and further discusses each topic.

During this seminar, Bhavan Dondapati, Vishal Vardhan Adepu, and Rohith Yogi Nomula presented [Fast & Furious: On the modelling of malware detection as an evolving data stream](https://www.sciencedirect.com/science/article/abs/pii/S0957417422016463). After their presentation our class had an open discussion related to the paper and more. This blog post will cover a summary of the information presented as well as a summary of our class discussion.

### Presentation Summary

---
#### Background
- The paper explores the impact of concept drift on malware classifiers with experiments using Android datasets
- Concept drift denotes how malware detectors struggle to identify new and evolving malware as a result of their changing characteristics
- Various strategies for mitigating malware are explored in this paper
- A novel data stream pipeline that combats concept drift is proposed and discussed

#### Data
- Android datasets are used:
  - Android is globally the most used operating system (more than 2 billion active devices monthly)
  - Surpassed Windows with a 40% market share
  - Malware attacks, evolution, and distribution are a result of the large usage
- DREBIN dataset:
  - Approximately 130,000 apps
  - Static features
  - 123,453 goodware and 5560 malware
- AndroZoo
  - Approximately 350,000 apps
  - Uses both static and dynamic features with the paper emphasizing on static features
  - Collected throughout 9 years from 2009-2018
  - 267,342 goodware and 80,102 malware
  - Temporal information is incorporated through the first seen date on VirusTotal
- These datasets are largely imbalanced

#### Threat Models and Assumptions
- The threat model considers an AV for Android due to its market leadership and how malware can affect a large number of users
- The detection model is completely static (features are retrieved directly from APK files)
  - Static malware detection is the most popular and fastes way to triage malware samples
  - The most prevalent approach for detecting malware within the Android environment
- The goal of this paper is to highlight the need for updating ML models based on classifiers, not to implement an actual AV
- Simulated the behavior of an online AV using offline experiments that use data streams

#### Data Stream
The proposed data stream pipeline considers the feature extractor under changes, on top of updating the classifier:
1. Obtain a new sample *X* from the raw data stream
2. Use feature extractor *E* to extract features from *X*, trained with previous data
3. Predict the class of *X* with classifier *C*, trained with previous data
4. With *C*'s prediction, update drift detector *D* to check the drift level
5. Based on the drift level, one of three steps can occur (all of them restart the pipeline):
   a. Normal: incrementally update *C* with *X*
   b. Warning: incrementally update *C* with *X* and add *X* to a buffer
   c. Drift: retrain both *E* and *C* using only the data collected during the warning level (from the buffer build during this level), creating a new extractor and classifier

#### Representation
- Uses two types of feature representation: Word2Vec and TF-IDF:
  - Both are widely used for text classification
  - Word2Vec:
    - Converts textual attributes of malware, such as API calls or system calls, into dense vectors
    - The vectors capture semantic similarities between attributes
  - TF-IDF:
    - Assigns weights to attributes based on their frequency within a document and across the corpus
    - Highlights the significance of terms within each sample
   
#### Concept Drift Detectors
- Drift Detection Method (DDM), Early Drift Detection Method (EDDM), ADaptive WINdowing (ADWIN), Kolmogorov - Smirnov WINdowing (KSWIN)
- DDM and EDDM
  - Online supervised methods based on sequential error monitoring (each incoming example is processed separately estimating the sequential error rate)
  - They assume that the increase of consecutive error rate suggests the occurrence of concept drift. DDM directly uses the error rate, while EDDM uses the distance-error rate, which measures the number of examples between two errors
  - Has two trigger levels: Warning and Drift
  - Warning level suggests that the concept starts to drift, then an alternative classifier is updated using the samples which rely on this level
  - The drift level suggests that the concept drift occurred, and the alternative classifier build during the warning level replaces the current classifier
 - ADWIN
   - Keeps statistics from sliding windows of variable size, used to compute the average of the change observed by cutting them in different points
   - If difference between two windows is greater than predefined threshold - concept drift
  - KSWIN
