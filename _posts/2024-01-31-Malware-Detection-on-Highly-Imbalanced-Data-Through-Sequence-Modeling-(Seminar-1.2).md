## Malware Detection on Highly Imbalanced Data through Sequence Modeling (Seminar 1.2)

**NOTE**: This blog is for a special topics course at Texas A&M (ML for Cyber Defenses). During each lecture a student presents information from the assigned paper. This blog summarizes and further discusses each topic.

During this seminar, students Akshat Punjabi and Akshat Pandey presented information from [Malware Detection on Highly Imbalanced Data through Sequence Modeling](https://dl.acm.org/doi/pdf/10.1145/3338501.3357374). After their presentation our class had an open discussion related to the paper and more. This blog post will cover a summary of the information discussed by both Akshats as well as a summary of our class discussion.

### Presentation Summary
This seminar covers dynamic analysis on sequences of Android OS malware detection activity. Below are the main ideas presented:

#### Static vs Dynamic Analysis
- Static analysis consists of analyzing an application without it being executed (program structure and used utilities)
- Dynamic analysis consists of analyzing an application in a sandbox based on actual API calls and execution
- This paper covers malware detection using dynamic analysis

#### Traditional Malware Detection Technique: Rule-based approach
- Uses explicit rules to classify software as benign or malicious
- Recognizes threats through known signatures and patterns
- Problems:
  - Tends to have a high FPR due to inconsistenices in system behavior
  - Hard to generate heuristics (best guesses or estimations) without ample domain knowledge
  - Easy for adversarial machine learning (attacker can disguise malware as benign within a sequence of actions)

#### Natural Language Processing Techniques for Malware Detection
- NLP approaches can significantly help malware detection as they both analyze sequences
- Long Short-Term Memory (LSTM)
  - Has already shown to be effective in log file analysis and anomaly detection (activity sequences are similar to language models)
  - A form of recurrent neural network that is good for processing and clustering long sequences of data
  - Incorporates previous input at the current input, combating the issue of forgetting information
  - Utilizes gates to retain essential information and discard unnecessary information
- Bidirectional Encoder Representations from Transformers (BERT)
  - Transformers use attention mechanisms for sequence modeling
  - Considers the whole sentence instead of individual words to develop context
  - Utilizes Masked Language Model (MLM) to randomly mask input and predict the masked word from context
  - Looks at words in sentences in both directions

#### Background on Datasets and Pre-processing
- Malware samples are rare which creates unbalanced datasets
- The paper uses Android OS activity sequences generated by WildFire
- Activities refer to any API call
- Machine learning models understand numbers, so a number is mapped to each action in the sequences

#### Approaches in Analyzing Android Activities
- N-gram
  - Processes a group of several activities/sequences together
  - N-grams are redundant and do not guarantee locality
- BERT
  - Using a pretrained BERT model works better than training from scratch
  - Performs well for Android malware detection
  - Learns sequence patterns that are relevant to Android maliciousness

#### Experiments
- Evaluation metrics include accuracy, precision, recall, and F_1 score

