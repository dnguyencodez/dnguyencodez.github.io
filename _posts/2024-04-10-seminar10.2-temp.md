### Discussion Summary

---
- There is no defined current baseline regarding AI/LLM performance
  - Comparing two different AI models is equivalent to comparing two different user groups (ex: comparing bad to good models and comparing groups of bad programmers to good programmers)
  - This paper proposes a unique user study: surveying users in regards to LLMs, something we are not used to
- Measuring complexity of software:
  - Typically lines of code
  - Measuring number of bugs per lines of code for robustness of security
- Measuring lines of code is not a good metric for LLMs
  - LLMs don't necessarily produce better code, but they are able to produce much more code
  - A question in research is how to define this metric for LLMs
- CWEs (Common Weakness Enumeration) vs CVEs (Common Vulnerabilities and Exposures)
  - CWE is a categorization of a weakness, a particular instance of the vulnerability
  - CVEs are cases reported that were actually exposed
- Bugs vs vulnerabilities: bugs can just be logical errors whereas vulnerabilities can be exploited by attackers
- Safety vs security:
  - Safety is about the functionality of your own system (such as fault tolerance) whereas security relates to external factors
  - Essentially, safety is concerned with the correctness of logic and design and security is concerned with protecting from external actors
