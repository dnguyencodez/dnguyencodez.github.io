### Discussion Summary

---
- There is a difference between helping a human complete the code and fully completing the code for the human
  - Ex: GitHub copilot cannot be fully autonomous in completing correct code
- The authors evaluate by using LLMs to analyze a decompiled source code (into binary code), then recompile into the original source code
  - The results are not fully optimal as you lose essential information when decompiling source code and recompiling
  - A systematic evaluation is not quite there yet
- Are LLMs able to understand the code that they are analyzing, provided by humans?
  - LLMs need a knowledge base of the code
  - There is no such knowledge layer yet, which is a research question that seeks to add a reasoning/inference layer to LLMs
  - Knowledge comes from training, so research wants to find a way to add an inference layer
- This is the first paper that proposes using LLMs for security
  - A big question is are these models able to reason on the data or are they being overfitted?
    - The general consensus is that they are being overfitted
  - Security is an outlier in the ML/AI domain
- In this research, they are evaluating the performance of LLMs using T/F questions
- The evaluation discussed in this paper is much less developed than previous papers (such as credibility of classifiers in detecting concept drift, malware, etc.)
  - This work is very recent, so there is a lot more progress to be made in analyzing LLMs for security
- What are problems in evaluating LLMs?
  - Interpretability: relating outputs of LLMs to other problems in security
  - Reproducability: the model provides a different answer every time. Models use probability given input words to determine the most probable response. Temperature also controls randomness of responses.
- You can craft a compiler in a certain to remove randomness
  - Example: making it fixed so that the same results are produced
  - Standard compilers like GCC are not like this
- LLMs also have a filter in front, similar to antivirus
  - The filter analyzes the input prompt to determine if it is ethical or safe to process
  - Don't want to have to retrain an entire model like GPT
- Why do some prompts produce better results than other?
  - LLMs do not separate instructions from data. Prompts are instructions while the data the LLM is trying to process is also in the prompt
  - Power of LLMs is the power of probability, and the probabilities are in the prompt
  - That is why it is hard to develop a way to separate the data and instruction architecture
